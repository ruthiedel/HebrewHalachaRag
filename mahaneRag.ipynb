{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mahane Rag** - Hebrew Jewish book QA  \n",
    "![Mahane Rag logo](logo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this project i attemped to make a halacha-chatbot.  \n",
    "for that, i used \"Yalkut Yosef\" books wich cover all Halacha topics and were written in the current century"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "frist - import all the necessary libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "import streamlit as st\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import cohere\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.load.dump import dumps\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import anthropic\n",
    "import ssl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Set a random seed for PyTorch (for GPU as well)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**text splitter**  \n",
    "now we need to make a function that will split the book into documents.  \n",
    "For this we have 2 important principles:  \n",
    "1. Each document contains a complete idea\n",
    "2. The document is short enough to produce good weights in the assimilation model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the RecursiveCharacterTextSplitter we need to make hierarchy aaray of the splitting characters\n",
    "MARKDOWN_SEPARATORS = [\n",
    "    \"\\n#{1,6} \",\n",
    "    \"```\\n\",\n",
    "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "    \"\\n---+\\n\",\n",
    "    \"\\n___+\\n\",\n",
    "    \"\\n\\n\",\n",
    "    \"\\n\",\n",
    "    \".\",\n",
    "    \" \",\n",
    "]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,#size of documents\n",
    "    chunk_overlap=80,#overlap of 2 nearby documents\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=MARKDOWN_SEPARATORS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , the embbeding function is needed  \n",
    "The embedding proccess is preformed only once   \n",
    "the embedded vectors are kept in the local mongoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Hebrew BERT model\n",
    "model_name = \"avichr/heBERT\"\n",
    "model = BertModel.from_pretrained(model_name, resume_download=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "#embedding function\n",
    "def embeddingVectors(text):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        text,  # List of input texts\n",
    "        padding=True,  # Pad to the maximum sequence length\n",
    "        truncation=True,  # Truncate to the maximum sequence length if necessary\n",
    "        return_tensors='pt',  # Return PyTorch tensors\n",
    "        add_special_tokens=True  # Add special tokens CLS and SEP\n",
    "    )\n",
    "    input_ids = encoding['input_ids']  # Token IDs\n",
    "    attention_mask = encoding['attention_mask']  # Attention mask\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state  # (batch_size, sequence_length, embedding_dim)\n",
    "        sentence_embedding = word_embeddings[:, 0, :]  \n",
    "    return sentence_embedding  # This contains the embeddings\n",
    "\n",
    "\n",
    "#save in mongo\n",
    "def save_embedding(word_embeddings,topic):\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"halacha_embedding\"]\n",
    "    embedded_texts_collection = db[f\"{topic}_embedded_vectors\"]\n",
    "    embedded_texts_collection.insert_many([{\"embedding\": we.tolist()} for we in word_embeddings])\n",
    "\n",
    "# embedding and saving proccess    \n",
    "def process_sentences(sentences,topic):\n",
    "    word_embeddings = embeddingVectors([doc[\"text\"] for doc in sentences])\n",
    "    save_embedding(word_embeddings,topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parent Document Retriever\n",
    "![Parent Document Retriever](PDR.png)\n",
    "Parent Document Retriever- the first splliting step  \n",
    "\n",
    "division of the book according to the division of the Jewish books of Halacha into chapters and sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataPreprocessing(content, topic):\n",
    "    current_title = None\n",
    "    \n",
    "    current_section = None\n",
    "    sections = []\n",
    "\n",
    "    section_letters = [\"א\", \"ב\", \"ג\", \"ד\", \"ה\", \"ו\", \"ז\", \"ח\", \"ט\", \"י\", \"יא\", \"יב\", \"יג\", \"יד\", \"טו\", \"טז\", \"יז\", \"יח\", \"יט\", \"כ\", \"כא\", \"כב\", \"כג\", \"כד\", \"כה\", \"כו\", \"כז\", \"כח\", \"כט\", \"ל\"]\n",
    "\n",
    "    for line in content:\n",
    "        line = line.strip()\n",
    "        line = re.sub(r'\\[[^\\]]*\\]', '', line)\n",
    "        line = re.sub(r'\\([^)]*\\)', '', line)\n",
    "        line = re.sub(r'\\.\\s\\.', '.', line)\n",
    "\n",
    "        if line.startswith(\"!\"):\n",
    "            current_title = line[1:].strip() + \".\"\n",
    "        elif any(line.startswith(letter) for letter in section_letters):\n",
    "            if current_section:\n",
    "                sections.append(current_section)\n",
    "            current_section = {\"title\": current_title, \"content\": line[1:].strip()}\n",
    "        else:\n",
    "            if current_section:\n",
    "                current_section[\"content\"] += \" \" + line.strip()\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(current_section)\n",
    "\n",
    "    processed_texts = []\n",
    "    for idx, section in enumerate(sections):\n",
    "        chunks = text_splitter.split_text(section[\"content\"])\n",
    "        for chunk in chunks:\n",
    "            document = {\n",
    "                \"metadata\": {\"title\": section[\"title\"], \"subIndex\": idx},\n",
    "                \"text\": section[\"title\"] + \" \" + chunk\n",
    "            }\n",
    "            processed_texts.append(document)\n",
    "  \n",
    "    process_sentences(processed_texts,topic)\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"halacha\"]\n",
    "    processed_texts_collection = db[f\"{topic}_processed_texts\"]\n",
    "    sections_collection = db[f\"{topic}_sections\"]\n",
    "\n",
    "    processed_texts_collection.insert_many(processed_texts)\n",
    "    sections_collection.insert_many(sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_mongodb(topic):\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"halacha\"]\n",
    "    processed_texts_collection = db[f\"{topic}_processed_texts\"]\n",
    "    sections_collection = db[f\"{topic}_sections\"]\n",
    "\n",
    "    processed_texts = list(processed_texts_collection.find())\n",
    "    sections = list(sections_collection.find())\n",
    "\n",
    "    return processed_texts, sections\n",
    "\n",
    "\n",
    "\n",
    "def load_vectors_from_mongodb(topic):\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"halacha_embedding\"]\n",
    "    embedded_texts_collection = db[f\"{topic}_embedded_vectors\"]\n",
    "    embeddings = list(embedded_texts_collection.find({}, {\"_id\": 0, \"embedding\": 1}))\n",
    "    return [torch.tensor(e[\"embedding\"]) for e in embeddings]\n",
    "\n",
    "\n",
    "\n",
    "def loadTopics():\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"halacha\"]\n",
    "    topics_collection = db[\"topics\"]\n",
    "    topics = list(topics_collection.find({}))\n",
    "    return [topic[\"name\"] for topic in topics]\n",
    "\n",
    "\n",
    "def saveTopic(topic):\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "    db = client[\"halacha\"]\n",
    "    topics_collection = db[\"topics\"]\n",
    "    topics_collection.insert_one({\"name\": topic})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another function that is needed is a ranking function because the cosine similarity is not accurate enough  \n",
    "i used a pretrained rerank model  from cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client('your-cohere-APIkey')\n",
    "def rerank_documents(query, docs, top_n):\n",
    "    results = co.rerank(model=\"rerank-multilingual-v3.0\", query=query, documents=docs, top_n=top_n, return_documents=True)\n",
    "    print(\"Rerank results:\", results)\n",
    "    reranked_docs = [res.document.text for res in results.results]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally - the Rag algorithem\n",
    "![RAG](rag.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sections_by_title(sections, title):\n",
    "    return [section[\"content\"] for section in sections if section[\"title\"] == title]\n",
    "\n",
    "\n",
    "boto3_bedrock = boto3.client(service_name='bedrock-runtime',region_name='us-east-1')\n",
    "\n",
    "\n",
    "def RAG(question, topic):\n",
    "    encoded_sentences = load_vectors_from_mongodb(topic)\n",
    "    encoded_sentences_tensor = torch.stack(encoded_sentences)  # המרת הרשימה לטנסור אחד\n",
    "\n",
    "    query = [question]\n",
    "    embedded_query = embeddingVectors(query)\n",
    "\n",
    "    numpy_embeddings = encoded_sentences_tensor.cpu().numpy()\n",
    "    index = faiss.IndexFlatL2(encoded_sentences_tensor.shape[1])\n",
    "    index.add(numpy_embeddings)\n",
    "\n",
    "    D, I = index.search(embedded_query.cpu().numpy(), k=20)\n",
    "\n",
    "    processed_texts, sections = load_data_from_mongodb(topic)\n",
    "    nearest_neighbors_texts = [processed_texts[idx][\"text\"] for idx in I[0]]\n",
    "\n",
    "    reranked_texts = rerank_documents(query[0], nearest_neighbors_texts, top_n=20)\n",
    "    first_rerank=\"\"\n",
    "    if reranked_texts:\n",
    "        first_text = reranked_texts[0]\n",
    "        first_period_index = first_text.find('.')\n",
    "        if first_period_index != -1 and first_period_index <= 50:\n",
    "            first_rerank = first_text[first_period_index+1:].strip()\n",
    "        else:\n",
    "            first_rerank = first_text    \n",
    "    processed_texts_2 = []\n",
    "    for text in reranked_texts:\n",
    "        first_period_index = text.find('.')\n",
    "        if first_period_index != -1 and first_period_index <= 50:\n",
    "            title = text[:first_period_index+1].strip()\n",
    "            relevant_sections = find_sections_by_title(sections, title)\n",
    "            combined_content = \" \".join(relevant_sections)\n",
    "            processed_texts_2.append(combined_content)\n",
    "        else:\n",
    "            processed_texts_2.append(text)\n",
    "    top_3_texts = processed_texts_2[:3]\n",
    "    context = \" \".join(top_3_texts)\n",
    "    client = anthropic.Client(api_key=\"claude-APIkey\")\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-2.1\",\n",
    "        system=\"Human: אתה רב שעונה על שאלות בהלכה. תסכם בשלוש-ארבע נקודות על פי המידע המצורף שבו יש תשובה לשאלה , אם אתה לא יודע, תכתוב איני יודע.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \" Context: \" + context + \" Question: \" + query[0]}\n",
    "        ],\n",
    "        max_tokens=400\n",
    "    )\n",
    "    answer = \"\"\n",
    "    for text_block in response.content:\n",
    "        answer += text_block.text\n",
    "    return answer,first_rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for Ui I used streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.image('logo.png', use_column_width=True)\n",
    "st.markdown(\"<h1 style='text-align: center;'>שאל אותי כל שאלה</h1>\", unsafe_allow_html=True)\n",
    "\n",
    "if 'topics' not in st.session_state:\n",
    "    st.session_state['topics'] =loadTopics()\n",
    "\n",
    "selected_topic = st.selectbox(\"בחר נושא\", st.session_state['topics'])\n",
    "question = st.text_input('', placeholder='הכנס את השאלה כאן', key='input', label_visibility='collapsed')\n",
    "col1, col2, col3 = st.columns([1, 0.5, 1])\n",
    "\n",
    "with col2:\n",
    "    submit_button = st.button('שלח')\n",
    "\n",
    "if submit_button:\n",
    "    if question:\n",
    "        placeholder = st.empty()\n",
    "        placeholder.text(\"LOADING\")\n",
    "\n",
    "        answer, source  = RAG(question, selected_topic) \n",
    "\n",
    "        placeholder.text(\"\")\n",
    "        st.markdown(f\"<div style='direction: rtl; text-align: right;'>מקור: {source}</div> </br>\", unsafe_allow_html=True)\n",
    "        st.markdown(f\"<div style='direction: rtl; text-align: right;'>תשובת מענה RAG: {answer}</div>\", unsafe_allow_html=True)\n",
    "    else:\n",
    "        st.markdown(\"<div style='direction: rtl; text-align: right;'>אנא הכנס שאלה.</div>\", unsafe_allow_html=True)\n",
    "\n",
    "uploaded_file = st.file_uploader(\"העלה קובץ טקסט\", type=[\"txt\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    new_topic = st.text_input(\"הכנס שם לנושא החדש\")\n",
    "\n",
    "    if new_topic:\n",
    "        content = uploaded_file.read().decode(\"utf-8\")\n",
    "        lines = content.split(\"\\n\")\n",
    "        processed_lines = []\n",
    "        for line in lines:\n",
    "            if line.startswith(\"סימן\"):\n",
    "                line = \"!\" + line\n",
    "            processed_lines.append(line)\n",
    "\n",
    "        DataPreprocessing(processed_lines, new_topic)\n",
    "        saveTopic(new_topic)\n",
    "        st.success(f\"הנושא '{new_topic}' נוסף בהצלחה!\")\n",
    "        st.experimental_rerun()  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
